{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Install required packages"
      ],
      "metadata": {
        "id": "hBTpkj_9t1S8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -q \\\n",
        "    requests>=2.32.4 \\\n",
        "    google-ai-generativelanguage==0.6.15 \\\n",
        "    langchain-text-splitters \\\n",
        "    langchain-community \\\n",
        "    langgraph \\\n",
        "    \"langchain[google-genai]\" \\\n",
        "    langchain-openai \\\n",
        "    langchain-core \\\n",
        "    pypdf \\\n",
        "    faiss-cpu"
      ],
      "metadata": {
        "id": "e0UN7Ozxtzd6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Packages"
      ],
      "metadata": {
        "id": "gFKL6HVMuJrO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "from langchain.chat_models import init_chat_model\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_core.vectorstores import InMemoryVectorStore\n",
        "\n",
        "api_key =\"GOOGLE_API_KEY\"\n",
        "if not os.environ.get(\"GOOGLE_API_KEY\"):\n",
        "  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(api_key)\n",
        "\n",
        "llm = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "crQg2LDUv2yf",
        "outputId": "402777c7-9380-4d28-abe2-1c7bc94cdcc7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AIzaSyCb42LV4-wKcTI_n744cwZt3cfFKHvDrCQ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "open_api_key = \"OPENAI_API_KEY\"\n",
        "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
        "  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(open_api_key)\n",
        "\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
        "vector_store = InMemoryVectorStore(embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZrKSCa_qyOcH",
        "outputId": "4bdc902c-58d5-48d5-bd90-75180565cd82"
      },
      "execution_count": 7,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "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··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gsZaEbc1Xnow",
        "outputId": "7da244a8-4d71-430e-c26f-39b20d6f92bd"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.embeddings import Embeddings\n",
        "from typing import List\n",
        "\n",
        "# Custom TF-IDF Embeddings class for LangChain\n",
        "class TfidfEmbeddings(Embeddings):\n",
        "  \"\"\"\n",
        "    Custom TF-IDF embeddings implementation for LangChain compatibility\n",
        "\n",
        "    Args:\n",
        "        vectorizer: Fitted TfidfVectorizer instance\n",
        "  \"\"\"\n",
        "  def __init__(self, vectorizer):\n",
        "      self.vectorizer = vectorizer\n",
        "\n",
        "  def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
        "      \"\"\"Convert list of documents to TF-IDF vectors\"\"\"\n",
        "      return self.vectorizer.transform(texts).toarray().tolist()\n",
        "\n",
        "  def embed_query(self, text: str) -> List[float]:\n",
        "      \"\"\"Convert single query to TF-IDF vector\"\"\"\n",
        "      return self.vectorizer.transform([text]).toarray()[0].tolist()\n"
      ],
      "metadata": {
        "id": "T5U6XqgwY6H8"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import FAISS\n",
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "#test\n",
        "pdf_paths = [\"/content/drive/MyDrive/cysh/2025.cl-1.1.pdf\",\n",
        "             \"/content/drive/MyDrive/cysh/2025.cl-2.5.pdf\"]\n",
        "\n",
        "documents = []\n",
        "for i, pdf_path in enumerate(pdf_paths):\n",
        "    loader = PyPDFLoader(pdf_path)\n",
        "    pages = loader.load()\n",
        "\n",
        "    documents.extend(pages)\n",
        "print(f\"Loaded {len(documents)} pages from {len(pdf_paths)} PDFs.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYmbWjuMzEJZ",
        "outputId": "ab635de6-1e7e-454a-a1ad-02c1d92d48d3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 47 pages from 2 PDFs.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split documents into chunks"
      ],
      "metadata": {
        "id": "6P6kItNYu4H6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "chunks = text_splitter.split_documents(documents)\n",
        "print(f\"Split into {len(chunks)} chunks.\")"
      ],
      "metadata": {
        "id": "7KqFD2AXWXAC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73cec165-f40b-4f06-ecfb-caa75a04fd69"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Split into 289 chunks.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Create TF-IDF vectorizer and embeddings"
      ],
      "metadata": {
        "id": "eUZHzLB-u8BS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "# Fit the vectorizer on all chunk texts\n",
        "texts = [doc.page_content for doc in chunks]\n",
        "vectorizer.fit(texts)\n",
        "embeddings = TfidfEmbeddings(vectorizer)\n",
        "\n",
        "# Create FAISS vector store with custom embeddings\n",
        "vector_store = FAISS.from_documents(chunks, embeddings)\n",
        "feature_names = vectorizer.get_feature_names_out()"
      ],
      "metadata": {
        "id": "ibJiH4_5Wa0a"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Keyword Extraction using TF-IDF and Semantic Search Function"
      ],
      "metadata": {
        "id": "_nMqRzDfu_PS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(text):\n",
        "    text = re.sub(r'[^\\w\\s]', '', text.lower())\n",
        "    tokens = text.split()\n",
        "    stopwords = set(['is', 'the', 'of', 'on', 'to', 'in', 'and', 'a', 'like', 'as', 'for', 'due'])\n",
        "    return [word for word in tokens if word not in stopwords]\n",
        "\n",
        "def extract_keywords(tfidf_matrix, feature_names, top_n=5):\n",
        "    \"\"\"\n",
        "    Extract keywords using TF-IDF scores\n",
        "\n",
        "    Args:\n",
        "        tfidf_matrix: Sparse matrix of TF-IDF scores\n",
        "        feature_names: Array of feature names from vectorizer\n",
        "        top_n (int): Number of top keywords to extract\n",
        "\n",
        "    Returns:\n",
        "        tuple: (global_keywords, document_keywords)\n",
        "    \"\"\"\n",
        "    global_tfidf = np.mean(tfidf_matrix.toarray(), axis=0)\n",
        "    top_global_indices = np.argsort(global_tfidf)[-top_n:][::-1]\n",
        "    global_keywords = [feature_names[i] for i in top_global_indices]\n",
        "\n",
        "    doc_keywords = []\n",
        "    for row in tfidf_matrix:\n",
        "        top_indices = np.argsort(row.toarray().flatten())[-top_n:][::-1]\n",
        "        doc_keywords.append([feature_names[i] for i in top_indices])\n",
        "\n",
        "    return global_keywords, doc_keywords\n",
        "\n",
        "tfidf_matrix = vectorizer.transform(texts)\n",
        "global_keywords, doc_keywords = extract_keywords(tfidf_matrix, feature_names)\n",
        "print(\"Global Hot Keywords:\", global_keywords)"
      ],
      "metadata": {
        "id": "upnFC5uaWeib",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea7597b1-ac74-4492-aa99-3b5b2e3588d4"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Global Hot Keywords: ['text', 'dotless', 'arabic', 'dotted', 'language']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def semantic_search(query, top_k=3):\n",
        "    \"\"\"\n",
        "    Perform semantic search on the document collection\n",
        "\n",
        "    Args:\n",
        "        query (str): Search query\n",
        "        top_k (int): Number of top results to return\n",
        "\n",
        "    Returns:\n",
        "        list: List of tuples containing (content, metadata)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        retriever = vector_store.as_retriever(search_kwargs={\"k\": top_k})\n",
        "        results = retriever.invoke(query)\n",
        "        return [(doc.page_content, doc.metadata) for doc in results]\n",
        "    except Exception as e:\n",
        "        print(f\"Error during search: {str(e)}\")\n",
        "        return []"
      ],
      "metadata": {
        "id": "H-SrJul9vhzV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def execute_search_with_keywords(query, top_k=3):\n",
        "    \"\"\"\n",
        "    Execute search and extract keywords from relevant documents\n",
        "\n",
        "    Args:\n",
        "        query (str): Search query\n",
        "        top_k (int): Number of results to return\n",
        "    \"\"\"\n",
        "    print(f\"\\nSemantic Search Results for Query: '{query}'\")\n",
        "    search_results = semantic_search(query, top_k)\n",
        "\n",
        "    # Display search results\n",
        "    for i, (content, metadata) in enumerate(search_results):\n",
        "        source_file = metadata.get('source', 'Unknown')\n",
        "        page_num = metadata.get('page', 'Unknown')\n",
        "        print(f\"Result {i+1} (Source: {source_file}, Page: {page_num}):\")\n",
        "        print(f\"{content[:200]}...\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "    # Extract keywords from the most relevant document\n",
        "    if search_results:\n",
        "        top_result_source = search_results[0][1]['source']\n",
        "\n",
        "        # Filter chunks for the relevant document\n",
        "        relevant_chunks = [doc for doc in chunks if doc.metadata['source'] == top_result_source]\n",
        "        relevant_texts = [doc.page_content for doc in relevant_chunks]\n",
        "\n",
        "        # Compute TF-IDF for the relevant document's chunks\n",
        "        relevant_tfidf_matrix = vectorizer.transform(relevant_texts)\n",
        "\n",
        "        # Extract keywords for the relevant document\n",
        "        doc_global_keywords, _ = extract_keywords(relevant_tfidf_matrix, feature_names, top_n=10)\n",
        "        print(f\"Hot Keywords for this document: {doc_global_keywords}\")"
      ],
      "metadata": {
        "id": "YIktW3mlYeLh"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Examples"
      ],
      "metadata": {
        "id": "4d8i6hi0vzN6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "execute_search_with_keywords(\"What is dotless arabic?\")\n",
        "execute_search_with_keywords(\"define Computational Linguistics\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WyEpzUScvnqN",
        "outputId": "5ea37fbb-7bcc-46f0-f098-5e6b8e79e233"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Semantic Search Results for Query: 'What is dotless arabic?'\n",
            "Result 1 (Source: /content/drive/MyDrive/cysh/2025.cl-2.5.pdf, Page: 27):\n",
            "6. Restoring Dots to Dotless Text\n",
            "This study introduces dotless Arabic text as an alternative representation to dotted text\n",
            "for Arabic NLP . For tasks requiring Arabic text as output, we need to conve...\n",
            "--------------------------------------------------\n",
            "Result 2 (Source: /content/drive/MyDrive/cysh/2025.cl-2.5.pdf, Page: 17):\n",
            "the entropy at the word level.\n",
            "From this table, it can be seen that English has a lower entropy at the character\n",
            "level as compared to dotted Arabic, but dotless Arabic has the lowest. However, dotless...\n",
            "--------------------------------------------------\n",
            "Result 3 (Source: /content/drive/MyDrive/cysh/2025.cl-2.5.pdf, Page: 2):\n",
            "lenges encountered in Arabic NLP . The inherent density of Arabic morphology often\n",
            "results in a considerably large vocabulary. However, dotless text could mitigate this by\n",
            "mapping many dotted words in...\n",
            "--------------------------------------------------\n",
            "Hot Keywords for this document: ['text', 'dotless', 'arabic', 'dotted', 'tokenization', 'language', 'vocabulary', 'dataset', 'size', 'al']\n",
            "\n",
            "Semantic Search Results for Query: 'define Computational Linguistics'\n",
            "Result 1 (Source: /content/drive/MyDrive/cysh/2025.cl-1.1.pdf, Page: 3):\n",
            "n-gram models of natural language.\n",
            "Computational Linguistics, 18(4):467–480.\n",
            "Church, Kenneth W. and Robert L. Mercer.\n",
            "1993. Introduction to the special issue on\n",
            "computational linguistics using large\n",
            "c...\n",
            "--------------------------------------------------\n",
            "Result 2 (Source: /content/drive/MyDrive/cysh/2025.cl-1.1.pdf, Page: 3):\n",
            "Gildea, Daniel and Daniel Jurafsky. 2002.\n",
            "Automatic labeling of semantic roles.\n",
            "Computational Linguistics, 28(3):245–288.\n",
            "https://doi.org/10.1162\n",
            "/089120102760275983\n",
            "Grosz, Barbara J. and Candace L. S...\n",
            "--------------------------------------------------\n",
            "Result 3 (Source: /content/drive/MyDrive/cysh/2025.cl-1.1.pdf, Page: 1):\n",
            "broader recognition through the journal’s visibility-enhancing efforts, opportunities to\n",
            "present at conferences sponsored by the Association for Computational Linguistics\n",
            "(ACL), as well as eligibility...\n",
            "--------------------------------------------------\n",
            "Hot Keywords for this document: ['computational', 'linguistics', 'ﬁeld', '1162', 'journal', 'new', 'research', 'doi', 'org', 'coli']\n"
          ]
        }
      ]
    }
  ]
}